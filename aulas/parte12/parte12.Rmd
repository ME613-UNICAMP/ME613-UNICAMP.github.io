---
title: "ME613 - Análise de Regressão"
author: Benilton S Carvalho - 1S2020
output:
  ioslides_presentation:
    widescreen: yes
subtitle: Parte 12
logo: ../logo-imecc.png
---


# Gráficos de Regressão Parcial


## Introdução

Vimos anteriormente:

* Gráfico dos resíduos versus variável preditora: podemos usar para checar presença de curvatura.

* Gráfico dos resíduos versus variável preditora não inclusa no modelo: decidir se deve ser incluída.

Problema: estes gráficos não mostram o efeito marginal de uma variável, dado que as demais já estão no modelo.

## Gráfico de regressão parcial

*Added-variable plots* ou *adjusted variable plots* fornecem informação sobre a importância marginal de $X_k$, considerando as demais variáveis já incluídas no modelo.

Para o efeito marginal de $X_k$, consideramos os resíduos da regressão de $Y$ nas demais variáveis e os resíduos da regressão de $X_k$ nas demais variáveis. 

O gráfico destes dois resíduos mostra a importância marginal de $X_k$ na redução da variabilidade do resíduo. E também pode fornecer informação sobre a natureza da relação marginal de $X_k$ com $Y$.


## Exemplo

Considere uma regressão múltipla de primeira ordem com duas variáveis preditoras: $X_1$ e $X_2$.

Queremos estudar o efeito de $X_1$, dado que $X_2$ já está no modelo.

* Fazemos a regressão $Y$ em $X_2$ e obtemos os resíduos: $e(Y\mid X_2)$.

* Fazemos a regressão de $X_1$ em $X_2$ e obtemos os resíduos: $e(X_1\mid X_2)$.

* Fazemos o gráfico de $e(Y\mid X_2)$ versus $e(X_1\mid X_2)$.

## Exemplo


<center>
<img src="figuras/added1.png" width=800>
</center>

## Exemplo: Salário de gerentes

Para cada gerente: média salarial anual nos últimos 2 anos ($X_1$), medida de aversão a risco ($X_2$) e valor do seguro de vida ($Y$).


```{r,echo=FALSE,fig.align='center',fig.height=4,fig.width=4}
dados <- read.table("dados/CH10TA01.txt")
names(dados) <- c("X1","X2","Y")
panel.cor <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits=digits)[1]
    txt <- paste(prefix, txt, sep="")
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt)
}
pairs(dados,upper.panel=panel.cor,lower.panel=panel.smooth)
```

## Exemplo: Salário de gerentes {.smaller}

```{r,echo=FALSE}
library(knitr)
modelo <- lm(Y ~ X1 + X2, data=dados)
kable(summary(modelo)$coef,col.names = c("Estimativa","Erro-Padrão","t","valor de p"))
```


```{r,echo=FALSE,fig.align='center',fig.height=3.5,fig.width=4,warning=FALSE,message=FALSE}
library(latex2exp)
plot(y=resid(modelo),x=dados$X1,ylab=TeX('$Y-\\hat{Y}$'),xlab=TeX('$X_1$'))
```

## Exemplo: Salário de gerentes {.smaller}

```{r,echo=TRUE}
modelo1 <- lm(Y ~ X2, data=dados)
y_x2 <- resid(modelo1)
modelo2 <- lm(X1 ~ X2, data=dados)
x1_x2 <- resid(modelo2)
```

```{r,echo=FALSE,fig.align='center',fig.height=3.5,fig.width=4,warning=FALSE,message=FALSE}
plot(y=y_x2,x=x1_x2,ylab=TeX('$e(Y| X_2)$'),xlab=TeX('$e(X_1| X_2)$'))
```


# *Outliers*

## Introdução


<center>
<img src="figuras/out1.png" width=800>
</center>


# *Outliers* em $Y$

## Resíduo Semi-studentizado

$$e_i^*=\frac{Y_i-\hat{Y}_i}{\sqrt{QME}}$$

Para $n$ grande, quando $|e_i^*|>4$ considera-se a $i$-ésima observação como *outlier*.

## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
dat = read.table('./dados/fat.txt')
colnames(dat) <- c("X1","X2","X3","Y")
X1 = dat[,1]
X2 = dat[,2]
X3 = dat[,3]
Y = dat[,4]
modelo1 <- lm(Y ~ X1 + X2,data=dat)
rstandard(modelo1)
```


## Matriz "chapéu" {.build .smaller}

$$\mathbf{H}_{n\times n} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$$

$$\hat{\mathbf{Y}}=\mathbf{H}\mathbf{Y}$$

$$\mathbf{e}=(\mathbf{I}-\mathbf{H})\mathbf{Y}$$

$$Var(\mathbf{e})=\sigma^2(\mathbf{I}-\mathbf{H})$$

$$Var(e_i)=\sigma^2(1-h_{ii})$$

$$h_{ii}=\mathbf{X}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}_i$$

$$\mathbf{X_i}_{p\times 1}=\left(
\begin{array}{c}
1\\
X_{i,1}\\
\vdots\\
X_{i,p-1}
\end{array}
\right)
$$

$$Cov(e_i,e_j)=-h_{ij}\sigma^2\quad\quad i\neq j$$


## Resíduo Studentizado

$$\widehat{Var(e_i)}=QME(1-h_{ii})$$

$$\widehat{Cov(e_i,e_j)}=-h_{ij}QME\quad\quad i\neq j$$

Resíduo studentizado:

$$r_i=\frac{e_i}{\sqrt{QME(1-h_{ii})}}$$

## Resíduo Studentizado com observação excluída

Se uma observação é muito discrepante, ela pode influenciar no ajuste.

Procedimento:

* excluir a $i$-ésima observação

* ajustar o modelo com as $n-1$ observações restantes

* obter $\hat{Y}_{i(i)}$: o valor predito para a $i$-ésima observação quando esta foi excluída no ajuste do modelo.


$$d_i=Y_i-\hat{Y}_{i(i)}=\frac{e_i}{1-h_{ii}}$$

Obs: quanto maior $h_{ii}$, maior será $d_i$, em comparação com $e_i$.

## Resíduo Studentizado com observação excluída


$$\widehat{Var(d_i)}=QME_{(i)}(1+ \mathbf{X}_i^T (\mathbf{X}_{(i)}^T\mathbf{X}_{(i)})^{-1}\mathbf{X}_i)=\frac{QME_{(i)}}{1-h_{ii}}$$

**Resíduo Studentizado com observação excluída**

$$t_i=\frac{d_i}{\sqrt{\widehat{Var(d_i)}}}\sim t_{n-p-1}$$

podemos calcular $t_i$ sem de fato fazer ajustes separados para cada observação excluída:

$$t_i=e_i\left[ \frac{n-p-1}{SQE(1-h_{ii})-e_i^2}\right]^{1/2}$$

## Resíduo Studentizado com observação excluída

A observação $i$ é um *outlier* se $|t_i|> t_{n-p-1}(1-\alpha/2n)$, utilizando Bonferroni.
 
 

## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
e <- resid(modelo1)
h <- hatvalues(modelo1)
t <- rstudent(modelo1)
round(data.frame("e"=e,"h"=h,"t"=t),3)
```

## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
alpha=0.10
n = dim(dat)[1]
p = length(coefficients(modelo1))
t_c <- qt(1-alpha/(2*n),df=n-p-1)
```

Se $|t_i|> `r round(t_c,2)`$, então $i$ é observação *outlier*.


# *Outliers* em $X$

## Matriz "chapéu"

* $0\leq h_{ii}\leq 1$

* $\sum_{i=1}^nh_{ii}=p$, lembrando que $p$ é o número de parâmetros no modelo, incluindo o intercepto.

* Alavanca (*leverage*): $h_{ii}$ mede a distância entre os valores de $X$ da $i$-ésima observação e os valores médios de $X$ para as $n$ observações.

## Exemplo

```{r,echo=TRUE}
X1 <- c(14,19,12,11)
X2 <- c(25,32,22,15)
Y <- c(301,327,246,187)
Xmatriz <- matrix(cbind(rep(1,length(X1)),X1,X2),ncol=3)
H <- Xmatriz %*% solve(t(Xmatriz)%*%Xmatriz) %*% t(Xmatriz)
hii <- diag(H)
cbind(X1,X2,hii)
```


## Exemplo

```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=5}
h <- c("h11=","h22=","h33=","h44=")
plot(x=X1,y=X2,xlim=c(10,20),ylim=c(10,35))
points(x=mean(X1),y=mean(X2),col="red")
text(x=X1,y=X2,labels=paste0(h,round(hii,3)),pos=3)
```

## Matriz "chapéu" {.build}

$\hat{\mathbf{Y}}=\mathbf{H}\mathbf{Y}$, portanto cada $\hat{Y}_i$ é uma combinação linear de todos os valores de $Y$ e o peso de cada $Y_i$ para o valor ajustado $\hat{Y}_i$ depende de $h_{ii}$.

Quanto maior $h_{ii}$, maior o peso de $Y_i$ em $\hat{Y}_i$.

$h_{ii}$ é uma função que depende apenas dos valores de $X$, portanto, mede o papel dos valores de $X$ na determinação da importância de cada $Y_i$ no valor ajustado $\hat{Y}_i$.

Quanto maior $h_{ii}$, menor a variância de $e_i$. Desta forma, quanto maior $h_{ii}$, mais próximo $\hat{Y}_i$ tenderá a estar de $Y_i$.

## Ponto de alavanca

Um valor alavanca $h_{ii}$ é considerado alto se é duas vezes maior que o valor de alavanca médio, denotado por $\hat{h}$:

$$\bar{h}=\frac{\sum_{i=1}^nh_{ii}}{n}=\frac{p}{n}$$

Desta maneira, observações em que $h_{ii}>2p/n$ são consideradas *outliers* com respeito aos valores de $X$.



## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
e <- resid(modelo1)
h <- hatvalues(modelo1)
t <- rstudent(modelo1)
round(data.frame("e"=e,"h"=h,"t"=t),3)
```

## Exemplo - Gordura corporal {.smaller}

```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=5}
p=length(coef(modelo1))
n=dim(dat)[1]
plot(x=dat$X1,y=dat$X2,ylim=c(40,60),xlim=c(10,35),xlab="X1",ylab="X2")
points(x=mean(dat$X1),y=mean(dat$X2),col="red")
points(x=dat$X1[h>2*p/n],y=dat$X2[h>2*p/n],col="green")
text(x=dat$X1,y=dat$X2,labels=1:dim(dat)[1],pos=2,cex=.5)
```

$2p/n = `r 2*p/n`$

# Observações influentes

## Introdução

Após identificar casos *outliers* com respeito a $Y$ e/ou $X$, o próximo passo é verificar se esses casos são *influentes*.

Uma observação é considerada influente se sua exclusão causa grandes mudanças na regressão ajustada.

## *DFFITS* - Influência em um único valor ajustado {.build}


A influência que a $i$-ésima observação tem no valor ajustado $\hat{Y}_i$ é medida por:

$$DFFITS_i=\frac{\hat{Y}_i-\hat{Y}_{i(i)}}{\sqrt{QME_{(i)}h_{ii}}}$$

O denominador é o desvio-padrão estimado de $\hat{Y}_i$.

$$Var(\hat{\mathbf{Y}})=\mathbf{H}Var(\mathbf{Y})\mathbf{H}^T=\mathbf{H}(\sigma^2\mathbf{I})\mathbf{H}^T$$

Como $\mathbf{H}=\mathbf{H}^T$ (simétrica) e $\mathbf{H}\mathbf{H}=\mathbf{H}$ (idempotente), temos que:

$$Var(\hat{\mathbf{Y}})=\sigma^2\mathbf{H}$$


## *DFFITS* - Influência em um único valor ajustado {.build}


$$DFFITS_i=e_i\left[\frac{n-p-1}{SQE(1-h_{ii}-e_i^2)}\right]^{1/2}\left(\frac{h_{ii}}{1-h_{ii}} \right)^{1/2}=t_i\left(\frac{h_{ii}}{1-h_{ii}} \right)^{1/2}$$

Se uma observação é um *outlier* em $X$ e tem alto valor de alavanca, $DFFITS$ tenderá a ter um alto valor.

Para pequenos conjuntos de dados, se $|DFFITS_i|>1$, a observação é considerada influente.

Para grandes conjuntos de dados, se $|DFFITS_i|>2\sqrt{p/n}$, a observação é considerada influente.

## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
format(dffits(modelo1),scientific = FALSE)
```

## *Cook's Distance* - Influência em todos os valores ajustados {.build}


Medida para avaliar a influência de uma observação $i$ no ajuste das $n$ observações:

$$D_i=\frac{\sum_{j=1}^n(\hat{Y}_j-\hat{Y}_{j(i)})^2}{p QME}$$

Em forma matricial:

$$D_i=\frac{(\hat{\mathbf{Y}}-\hat{\mathbf{Y}}_{(i)})^T(\hat{\mathbf{Y}}-\hat{\mathbf{Y}}_{(i)})}{pQME}$$

## *Cook's Distance* - Influência em todos os valores ajustados {.build}

$$D_i=\frac{e_i^2}{pQME}\left[\frac{h_{ii}}{(1-h_{ii})^2}\right]$$

Comparamos $D_i$ com percentis de $F(p,n-p)$: se for maior que o percentil 50, a $i$-ésima observação deve ser investigada como possível ponto influente.


## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
format(cooks.distance(modelo1),scientific = FALSE)
```


## Exemplo - Gordura corporal 

```{r,echo=TRUE}
caso3 <- cooks.distance(modelo1)[3]
p=length(coefficients(modelo1))
n=dim(dat)[1]
perc=pf(caso3,df1=p,df2=n-p)
```

Caso $3$ tem o maior valor: $D_3=`r round(cooks.distance(modelo1)[3],3)`$. Este valor corresponde ao percentil `r round(100*perc,1)` da distribuição $F(p,n-p)$.

## $DFBETAS$ - influência nos coeficientes da regressão {.build}

Medida de influência da $i$-ésima observação no coeficiente $\hat{\beta}_k$:

$$DFBETAS_{k(i)}=\frac{\hat{\beta}_k-\hat{\beta}_{k(i)}}{\sqrt{QME_{(i)}c_{kk}}}$$

em que $c_{kk}$ é o $k$-ésimo elemento da diagonal de $(\mathbf{X}^T\mathbf{X})^{-1}$.

Para pequenos conjuntos de dados, se $|DFBETAS_{k(i)}|>1$, a observação é considerada influente.

Para grandes conjuntos de dados, se $|DFBETAS_{k(i)}|>2/\sqrt{n}$, a observação é considerada influente.


## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
format(dfbetas(modelo1),scientific = FALSE)
```


## Exemplo - Gordura corporal {.smaller}

```{r,echo=TRUE}
influence.measures(modelo1)
```


# Inflação da variância

## Introdução {.build}

Problemas quando variáveis preditoras apresentam correlação alta entre si:

* Incluir ou excluir uma variável preditora altera os coeficientes da regressão.

* Os erros padrão dos coeficientes estimados ficam muito grandes.

> A presença de multicolinearidade pode ser investigada através dos seguintes diagnósticos informais:

* Grandes mudanças nas estimativas dos parâmetros de regressão quando uma variável é incluída ou exclluída do modelo.

* Estimativa dos parâmetros de regressão com sinal oposto do que seria esperado, segundo informações/conhecimento prévio.


## Exemplo - Gordura corporal

```{r,echo=FALSE}
modelo1 <- lm(Y ~ X1 + X2,data=dat)
kable(summary(modelo1)$coef,col.names = c("Estimativa","Erro-Padrão","t","valor de p"))
```


## Exemplo - Gordura corporal

```{r,echo=FALSE}
modelo2 <- lm(Y ~ X1 + X2 + X3,data=dat)
kable(summary(modelo2)$coef,col.names = c("Estimativa","Erro-Padrão","t","valor de p"))
```


## $VIF$ - fator de inflação da variância {.build}

Lembrando:

$$Var(\hat{\boldsymbol{\beta}})=\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$$

> Para medir o impacto da multicolinearidade, é mais útil trabalhar com as variáveis com transformação de correlação, vistas anteriormente.

$$Var(\hat{\boldsymbol{\beta}^*})=(\sigma^*)^2\mathbf{r}_{XX}^{-1}$$

> Definindo $VIF_k$ (fator de inflação da variância para $\hat{\beta_k^*}$) como o $k$-ésimo elemento de $\mathbf{r}_{XX}^{-1}$, temos:

$$Var(\hat{\beta_k^*})=(\sigma^*)^2VIF_k$$

## $VIF$ - fator de inflação da variância {.build}

Pode-se reescrever:

$$VIF_k=(1-R_k^2)^{-1}$$

em que $R_k^2$ é o coeficiente de determinação da regressão de $X_k$ nas demais variáveis preditoras.

Quando $R_k^2=0$, $VIF_k=1$, caso contrário, $VIF_k>1$.

## Exemplo - Gordura corporal

Variáveis escala original

```{r,echo=TRUE}
modelo2 <- lm(Y ~ X1 + X2 + X3,data=dat)
kable(summary(modelo2)$coef,col.names = c("Estimativa","Erro-Padrão","t","valor de p"))
```


## Exemplo - Gordura corporal

Variáveis padronizadas

```{r,echo=FALSE}
dadosPadrao <- as.data.frame(scale(dat)/sqrt(dim(dat)[1]-1))
modeloPadrao <- lm(Y ~ X1+X2+X3-1,data=dadosPadrao)
kable(summary(modeloPadrao)$coef,col.names = c("Estimativa","Erro-Padrão","t","valor de p"))
```


## Exemplo - Gordura corporal

```{r,echo=TRUE}
library(car)
vif(modelo2)
```

## Agradecimento

* Slides criados por Samara F Kiihl / IMECC / UNICAMP


## Leitura

* Applied Linear Statistical Models: Capítulo 10.

* Faraway - [Linear Models with R](http://www.maths.bath.ac.uk/~jjf23/LMR/): Capítulo 6, Seção 7.3 

* Draper & Smith - [Applied Regression Analysis](http://onlinelibrary.wiley.com/book/10.1002/9781118625590): Capítulo 8.

* Caffo - [Regression Models for Data Science in R](https://leanpub.com/regmods): Residuals, variation, diagnostics.



<center>
<img src="figuras/sad-laff-working-from-home-joke1-e1392532216836.jpg" width=800>
</center>
